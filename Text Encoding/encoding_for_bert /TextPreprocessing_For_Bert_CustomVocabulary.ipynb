{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdqPpUBTI8o5XC0rYrrZDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NITHISHM2410/Text_Processing/blob/NLP/Text%20Encoding/encoding_for_bert%20/TextPreprocessing_For_Bert_CustomVocabulary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE3GbNOO-hib",
        "outputId": "1fc9d685-2791-422f-d518-d35500cf8017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text) (3.2.2)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.12.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "!pip install tensorflow-text\n",
        "import tensorflow_text as text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\")"
      ],
      "metadata": {
        "id": "2PlLgFPL-xIG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Module can be used for scenarios where we need to train BERT models on our custom vocabulary instead of BERT inbuilt vocabulary."
      ],
      "metadata": {
        "id": "Bun_xecoAkth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertED(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab, max_len):\n",
        "        super(BertED, self).__init__()\n",
        "        self.vocab = self.get_vocab(vocab)\n",
        "        self.maxlen = max_len\n",
        "        self.encode = tf.keras.layers.TextVectorization(\n",
        "            max_tokens=1000,\n",
        "            output_mode='int',\n",
        "            vocabulary=self.vocab,\n",
        "            standardize='lower_and_strip_punctuation'\n",
        "        )\n",
        "        self.decode = tf.keras.layers.StringLookup(\n",
        "            max_tokens=1000,\n",
        "            output_mode='int',\n",
        "            vocabulary=self.vocab,\n",
        "            invert=True\n",
        "        )\n",
        "\n",
        "    def get_vocab(self,vocab):\n",
        "        with open(vocab) as f:\n",
        "            vocab = f.read()\n",
        "        return vocab.split(\"\\n\")    \n",
        "\n",
        "\n",
        "    def pad(self, inputs):\n",
        "        inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            inputs,\n",
        "            maxlen=self.maxlen,\n",
        "            dtype='int32',\n",
        "            padding='post',\n",
        "            truncating='post',\n",
        "            value=0\n",
        "        )\n",
        "        return tf.convert_to_tensor(inputs)\n",
        "\n",
        "    def mask(self, input_tensor):\n",
        "        mask_tensor = tf.where(tf.equal(input_tensor, 0), tf.fill(tf.shape(input_tensor), 0), tf.ones_like(input_tensor))\n",
        "        return mask_tensor\n",
        "\n",
        "    def typeids(self, input):\n",
        "        return tf.zeros_like(input, dtype=tf.int32)\n",
        "\n",
        "    def create_dict(self, input):\n",
        "        sample = dict()\n",
        "        sample['input_word_ids'] = tf.cast(self.pad(self.encode(input)), tf.int32)\n",
        "        sample['input_mask'] = self.mask(sample['input_word_ids'])\n",
        "        sample['input_type_ids'] = self.typeids(sample['input_word_ids'])\n",
        "        return sample\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [self.create_dict(input) for input in inputs]\n",
        "        return inputs\n",
        "\n",
        "    def decoder(self, input):\n",
        "        output = self.decode(input)\n",
        "        cond = tf.math.logical_not(tf.equal(output, \"[UNK]\"))\n",
        "        output = tf.boolean_mask(output, cond)\n",
        "        return output\n",
        "    def back_to_string(self,inputs):\n",
        "        outputs = [self.decoder(input) for input in inputs]\n",
        "        return outputs\n",
        "         \n",
        "\n",
        "    def return_vocab(self):\n",
        "        return self.encode.get_vocabulary()\n"
      ],
      "metadata": {
        "id": "VQEFp3v3-yuL"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE YOUR CUSTOM VOCABULARY IN 'vocab' PARAMETER AND MAXIMUM LENGTH OF INPUT SENTENCE IN 'max_len' PARAMETER.\n"
      ],
      "metadata": {
        "id": "kLeoeHti_RAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = BertED(vocab = \"vocab.txt\", # Add your vocab\n",
        "                         max_len = 10,\n",
        "                    )"
      ],
      "metadata": {
        "id": "n6uOVUut--Ng"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASSING FEW INPUT SENTENCES TO ENCODE BASED ON OUR CUSTOM VOCAB"
      ],
      "metadata": {
        "id": "-XUNofCEBrJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.convert_to_tensor(\n",
        "        [[tf.Variable(['day and night']),tf.Variable(['car and bike'])],\n",
        "         [tf.Variable(['day and night']),tf.Variable(['car and bike'])]]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "dVokMFjxHNab"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tSZxcsiHlCO",
        "outputId": "9bc05935-9d80-4b92-859d-1425c51b7a8d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2, 1), dtype=string, numpy=\n",
              "array([[[b'day and night'],\n",
              "        [b'car and bike']],\n",
              "\n",
              "       [[b'day and night'],\n",
              "        [b'car and bike']]], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = preprocess(inputs)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvkbjnKw_hle",
        "outputId": "145d9cb9-7632-4280-c0c2-233790b2bb0d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_word_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[ 2,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [10,  1, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
              "  'input_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>},\n",
              " {'input_word_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[ 2,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
              "         [10,  1, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>,\n",
              "  'input_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VOCABULARY SET"
      ],
      "metadata": {
        "id": "Cy3O3ZGSBzHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess.return_vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqAuagWvAPuS",
        "outputId": "c65a40a2-bc60-4b36-826e-b974c46f68f3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'day',\n",
              " 'night',\n",
              " 'father',\n",
              " 'mother',\n",
              " 'like',\n",
              " 'love',\n",
              " 'today',\n",
              " 'after',\n",
              " 'car',\n",
              " 'tomorrow',\n",
              " 'how',\n",
              " 'bike',\n",
              " 'are',\n",
              " 'you',\n",
              " 'i',\n",
              " 'call',\n",
              " 'me',\n",
              " 'afternoon']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRYING TO DECODE USING THE SAME MODULE"
      ],
      "metadata": {
        "id": "LUyLT43sB2A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in results:\n",
        "    print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_9Vf1y9B-q0",
        "outputId": "ca51dd78-1ffd-4985-8ff1-c833f9bf4c84"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_word_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[ 2,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
            "       [10,  1, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n",
            "{'input_word_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[ 2,  1,  3,  0,  0,  0,  0,  0,  0,  0],\n",
            "       [10,  1, 13,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRYING TO PASS ENCODED SENTENCES TO MODEL"
      ],
      "metadata": {
        "id": "LBE22kxqDiDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in results:\n",
        "    print(model(batch)['pooled_output'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27zoRvkLDnpG",
        "outputId": "d5c65c5e-efc6-46c5-a9e2-7f4c16a27abb"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 512)\n",
            "(2, 512)\n"
          ]
        }
      ]
    }
  ]
}