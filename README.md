# TEXT PROCESSING 

  
  
 
# => Text Cleaning Class
Cleans the data by applying transformations :  

            - converting to lower case  
            - removing punctuations  
            - removing tags  
            - and many more
            
           
           
 # => Text Encoding Class
Instead of encoding text using the BERT vocabulary, this function encodes text for the BERT model and models based on custom vocabulary. When we wish to forecast tokens by utilising the softmax layer as the last layer, this can help reduce the amount of computational expense that is associated with NLP models. When compared to the BERT vocabulary, which contains around 30,000 terms, the use of bespoke vocabulary will result in a reduction in the overall size of the vocabulary.
           
            
      
 
